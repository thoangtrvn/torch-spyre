/*
 * Copyright 2025 The Torch-Spyre Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "spyre_mem.h"

#include <ATen/EmptyTensor.h>
#include <ATen/detail/PrivateUse1HooksInterface.h>
#include <ATen/native/Resize.h>
#include <ATen/ops/as_strided_cpu_dispatch.h>
#include <ATen/ops/set_cpu_dispatch.h>
#include <c10/core/Allocator.h>
#include <c10/core/MemoryFormat.h>
#include <c10/core/TensorOptions.h>
#include <c10/util/ArrayRef.h>
#include <torch/library.h>
#include <util/sen_data_convert.h>

#include <algorithm>
#include <cassert>
#include <memory>
#include <sendnn/graph/graph_builder.hpp>
#include <sendnn/interface/graph_loader.hpp>
#include <sendnn/runtime/runtime_interface.hpp>
#include <sendnn/tensor/sentensor_info.hpp>
#include <sendnn/util/status.hpp>
#include <string>
#include <utility>
#include <vector>

#include "logging.h"
#include "module.h"
#include "spyre_sendnn_utils.h"
#include "spyre_storage_impl.h"
#include "spyre_stream.hpp"
#include "spyre_tensor_impl.h"
#include "types_mapping.h"

namespace spyre {

using DataConversionStrideInfo = data_conversion_stride_info;
using DataConversionInfo = data_conversion_info;

/*
 * CPU stride for a dimension.
 *
 * @param dim: dimension index
 * @param stick_size: stick length for the dtype
 * @param dev_dim_order: order of tensor dimensions on device
 * @param cpu_strides: strides of cpu tensor
 * @return CPU stride of the dimension
 */
auto get_dim_cpu_stride(int dim, int stick_size,
                        std::vector<int32_t> dev_dim_order,
                        std::vector<int64_t> cpu_strides) {
  int cpu_stride;
  if (dim == dev_dim_order.back()) {  // stick_dim
    cpu_stride = stick_size;
  } else {
    cpu_stride = cpu_strides[dim];
  }
  return cpu_stride;
}
/*
 * Device stride for a dimension.
 *
 * @param dim: dimension index
 * @param stick_size: stick length for the dtype
 * @param dev_dim_order: order of tensor dimensions on device
 * @param dev_strides: strides of device tensor
 * @param dev_shape: shape of tensor on device
 * @return device stride of the dimension
 */
auto get_dim_device_stride(int dim, int stick_size,
                           std::vector<int64_t> dev_size,
                           std::vector<int64_t> dev_strides) {
  int dev_stride;
  if (dev_strides.size() == 1) {
    dev_stride = stick_size;
  } else {
    dev_stride = dev_strides.back() * dev_size[dev_strides.size() - 1];
  }
  return dev_stride;
}

/*
 * Fills out size and strides for each dimension of the tensor.
 *
 * @param sizes: dimension sizes of the CPU tensor
 * @param strides: dimension strides of the CPU tensor
 * @param stick_size: stick length for the dtype
 * @param device_sizes: dimesion sizes of dev tensor
 * @param host2device: direction of data conversion
 * @return description of data conversion
 */
auto get_device_stride_info(c10::IntArrayRef sizes, c10::IntArrayRef strides,
                            SpyreTensorLayout stl, int stick_size,
                            std::vector<int64_t> device_sizes, bool host2device)
    -> DataConversionStrideInfo {
  DataConversionStrideInfo stride_info;
  auto cpu_shape = sizes.vec();
  auto cpu_strides = strides.vec();

  // sparse tensors need no padding of the stick dimension
  bool sparse = stl.dim_map.back() == -1;
  bool requires_padding =
      !sparse && cpu_shape[stl.dim_map.back()] % stick_size != 0;
  bool size_less_than_stick =
      !sparse && cpu_shape[stl.dim_map.back()] < stick_size;

  stride_info.size_ = device_sizes;

  if (size_less_than_stick) {
    stride_info.size_[0] = cpu_shape[stl.dim_map.back()];
  }
  stride_info.stride_src_.push_back(1);
  stride_info.stride_dst_.push_back(1);

  for (int i = 1; i < stl.dim_map.size(); i++) {
    auto& dim = stl.dim_map[(stl.dim_map.size() - 1) - i];
    auto cpu_stride =
        get_dim_cpu_stride(dim, stick_size, stl.dim_map, cpu_strides);
    auto dev_stride = get_dim_device_stride(
        dim, stick_size, device_sizes,
        host2device ? stride_info.stride_dst_ : stride_info.stride_src_);

    stride_info.stride_src_.push_back(host2device ? cpu_stride : dev_stride);
    stride_info.stride_dst_.push_back(host2device ? dev_stride : cpu_stride);
    if (dim == stl.dim_map.back() && requires_padding &&
        !size_less_than_stick) {  // stick_dim
      stride_info.size_[i] -= 1;
    }
  }
  stride_info.offset_src_ = 0;
  stride_info.offset_dst_ = 0;

  // pull single value from stick if sparse tensor
  if (sparse) {
    stride_info.size_[0] = 1;
  }

  return stride_info;
}
/*
 * Generates one or more descriptions of data conversions based on padding
 * requirements.
 *
 * The stick dimension must be a multiple of the stick size. If the size of this
 * dimension on the CPU is not a multiple of the stick size, then padding is
 * added during the data conversion step. This padding is handled in two
 * different ways based on the size of the dimension:
 *    1. If the size of the stick dimension is less than the stick size, then
 *     a single DataConversionStrideInfo struct is created with the size of
 * that dimension being the cpu shape.
 *    2. If the size of the stick dimension is more than the stick size, then
 * two DataConversionStrideInfo are needed. The first is has the size of the
 * stick dimension being the cpu shape. The cpu and device offsets are 0. The
 * second DataConversionStrideInfo has the same cpu and device strides as the
 * first. For the second, the size of the stick dimension is the remainder of
 * the dimension size divided by the stick size (rounded down). The cpu offset
 * is the dimension size divided by the stick size (rounded up), multiplied by
 * the stick size. The device offset is the size of the stick size multiplied by
 * the volume of the dimensions preceeding the stick dim on the device.
 *
 * @param sizes: dimension sizes of the CPU tensor
 * @param strides: dimension strides of the CPU tensor
 * @param dev_shape: shape of tensor on device
 * @param host2device: direction of data conversion
 * @return descriptions of data conversions for the tensor
 */
auto get_device_stride_infos(c10::IntArrayRef sizes, c10::IntArrayRef strides,
                             SpyreTensorLayout stl,
                             std::vector<int64_t> dev_shape, bool host2device)
    -> std::vector<DataConversionStrideInfo> {
  std::vector<DataConversionStrideInfo> dcsi;
  auto cpu_shape = sizes.vec();
  int stick_size = stl.elems_per_stick();

  // sparse tensors need no padding of the stick dimension
  bool sparse = stl.dim_map.back() == -1;
  bool requires_padding =
      !sparse && cpu_shape[stl.dim_map.back()] % stick_size != 0;
  bool size_less_than_stick =
      !sparse && cpu_shape[stl.dim_map.back()] < stick_size;
  DataConversionStrideInfo stride_info;

  stride_info = get_device_stride_info(sizes, strides, stl, stick_size,
                                       dev_shape, host2device);
  dcsi.push_back(stride_info);

  if (requires_padding && !size_less_than_stick) {
    /* Second DataConversionStrideInfo has same strides, so we can reuse the
     * stride information from the first DataConversionStrideInfo
     * and update the stick dim sizes and offsets
     */
    auto pad_stride_info = stride_info;
    auto dev_offset = stick_size;
    auto cpu_offset = stick_size;

    // Update host and device offsets
    for (int i = 1; i < stl.dim_map.size(); i++) {
      auto& dim = stl.dim_map[(stl.dim_map.size() - 1) - i];
      dev_offset *= pad_stride_info.size_[i];
      if (dim == stl.dim_map.back()) {
        cpu_offset *= pad_stride_info.size_[i];
        // Stick dimension is the size of the remainder of cpu_shape/stick_size
        pad_stride_info.size_[i] = 1;
        pad_stride_info.size_[0] = cpu_shape[stl.dim_map.back()] % stick_size;
        break;
      }
    }
    pad_stride_info.offset_src_ = host2device ? cpu_offset : dev_offset;
    pad_stride_info.offset_dst_ = host2device ? dev_offset : cpu_offset;
    dcsi.push_back(pad_stride_info);
  }
  return dcsi;
}
/*
 * Generate description of data conversion for a tensor.
 *
 * @param tensor: tensor to convert
 * @return data conversion information in string
 */
auto generate_dci_(const at::Tensor* tensor, SpyreTensorLayout stl,
                   bool host2device) -> DataConversionInfo {
  /*   host2device = true : then 'tensor' is CPU-tensor
   *   host2device = false: then 'tensor' is Spyre-tensor
   */
  auto str_type = torchScalarToString[tensor->scalar_type()];
  const auto [dtype_cpu, dtype_dev] = stringToDTDataFormatPair(str_type);
  std::stringstream s;
  auto cpu_shape = tensor->sizes().vec();
  DataConversionInfo dci{};
  dci.dci_dsName_ = "DCI-Tensor-0";
  dci.isHostToSen_ = host2device;
  dci.dataformat_src_ = host2device ? dtype_cpu : dtype_dev;
  dci.dataformat_dst_ = host2device ? dtype_dev : dtype_cpu;
  // Reverse PyTorch ordering
  auto dev_shape = stl.device_size;
  std::reverse(cpu_shape.begin(), cpu_shape.end());
  std::reverse(dev_shape.begin(), dev_shape.end());
  dci.dcsi_ = get_device_stride_infos(tensor->sizes(), tensor->strides(), stl,
                                      dev_shape, host2device);

  dci.input_shape_ = host2device ? cpu_shape : dev_shape;
  dci.output_shape_ = host2device ? dev_shape : cpu_shape;
  return dci;
}

auto generate_dci(const at::Tensor* tensor, SpyreTensorLayout stl,
                  bool host2device) -> std::string {
  std::stringstream s;
  generate_dci_(tensor, stl, host2device).exportJson(s);
  DEBUGINFO("DataConversionInfo: ", s.str());
  return s.str();
}

// A custom allocator for our custom device, what returns is a handle to the
// allocated memory not the actual pointer
struct SpyreAllocator final : public at::Allocator {
 private:
  SpyreAllocator() = default;
  flex::DeviceMemoryAllocatorPtr getAllocator(unsigned int dev_id) {
    return GlobalRuntime::get()
        ->GetDeviceHandle(dev_id)
        ->GetDeviceMemoryAllocator();
  }

 public:
  static SpyreAllocator& instance() {
    static SpyreAllocator allocator;
    return allocator;
  }

  at::DataPtr allocate(size_t nbytes) override {
    c10::Device curr_device =
        c10::impl::getDeviceGuardImpl(c10::DeviceType::PrivateUse1)
            ->getDevice();

    auto device_id = curr_device.index();
    DEBUGINFO("allocating ", nbytes, " (bytes) on Spyre", curr_device);
    if (nbytes <= 0) {
      return {nullptr, nullptr, &ReportAndDelete, curr_device};
    }
    auto allocator = getAllocator(device_id);
    flex::DeviceMemoryAllocationPtr data;  // a smart-pointer object
    // NOTE: last argument should be set to 0
    allocator->TryAllocate(&data, nbytes, 0);
    TORCH_CHECK(data, "Failed to allocate ", nbytes, " bytes on Spyre device.");
    auto* ctx = new SharedOwnerCtx{std::move(data), device_id};
    void* ctx_void = static_cast<void*>(ctx);

    void* data_void = static_cast<void*>(ctx->owner.get());

    auto data_ptr_result =
        at::DataPtr(data_void, ctx_void, &ReportAndDelete, curr_device);

    return data_ptr_result;
  }

  static void ReportAndDelete(void* ctx_void) {
    if (!ctx_void) {
      return;
    }
    auto* ctx = static_cast<SharedOwnerCtx*>(ctx_void);
    delete ctx;
  }

  // The raw deleter only gets passed the data ptr, no context, so
  // it would not work right now. To implement this, we first need to
  // create a runtime interface that can correctly free an allocation
  // only based on the data ptr, without the allocation idx from the
  // context
  at::DeleterFnPtr raw_deleter() const override {
    return nullptr;
  }

  void copy_data(void* dest, const void* src, std::size_t count) const final {
    py::gil_scoped_acquire acquire;
    DEBUGINFO("entering allocator->copy_data method");
    // do nothing -- look into when this is called
    // spyre_copy_from(reinterpret_cast<spyre_ptr_t>(dest),
    // reinterpret_cast<spyre_ptr_t>(src));
  }
};

// Register our custom allocator
REGISTER_ALLOCATOR(c10::DeviceType::PrivateUse1, &SpyreAllocator::instance());

// Empty op needs C++ code and cannot be handled by python side fallback
at::Tensor spyre_empty(c10::IntArrayRef size,
                       std::optional<c10::ScalarType> dtype_opt,
                       std::optional<c10::Layout> layout_opt,
                       std::optional<c10::Device> device_opt,
                       std::optional<bool> pin_memory_opt,
                       std::optional<c10::MemoryFormat> memory_format_opt) {
  c10::Device device = device_opt.value_or(
      c10::impl::VirtualGuardImpl{c10::DeviceType::PrivateUse1}.getDevice());
  DEBUGINFO("shape=", size, " on Spyre ", device);
  const auto dtype = c10::dtype_or_default(dtype_opt);
  TORCH_CHECK(device.is_privateuseone());
  TORCH_CHECK(c10::layout_or_default(layout_opt) == c10::Layout::Strided,
              "Non strided layout not supported");
  TORCH_CHECK(!c10::pinned_memory_or_default(pin_memory_opt),
              "Pin memory can only be on CPU");
  const c10::DeviceGuard device_guard(device);

  auto device_layout = SpyreTensorLayout(size.vec(), dtype);
  size_t size_bytes = get_device_size_in_bytes(device_layout);
  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);
  auto tensor = at::detail::make_tensor_base<SpyreTensorImpl>(
      c10::Storage(c10::make_intrusive<SpyreStorageImpl>(
          c10::StorageImpl::use_byte_size_t(), size_bytes,
          &SpyreAllocator::instance(),
          /*resizeable=*/true)),
      pu1_dks, c10::scalarTypeToTypeMeta(dtype));

  tensor.unsafeGetTensorImpl()->set_sizes_contiguous(size);
  static_cast<SpyreTensorImpl*>(tensor.unsafeGetTensorImpl())->spyre_layout =
      device_layout;
  DEBUGINFO("SpyreTensorLayout: ", device_layout.toString());
  return tensor;
}

/**
 * This method will determine the size of the tensor on Spyre, then allocate
 * that space on the Spyre and and set the handle for the tensor to that of the
 * memory in the Spyre. For now, it allocates a CPU tensor with the correct
 * size, as the actual storage will stay on CPU until the rest of the stack is
 * ready to filter out the allocation and deallocation of memory from the graph
 * processing.
 */
at::Tensor spyre_empty_strided(c10::IntArrayRef size, c10::IntArrayRef stride,
                               std::optional<c10::ScalarType> dtype_opt,
                               std::optional<c10::Layout> layout_opt,
                               std::optional<c10::Device> device_opt,
                               std::optional<bool> pin_memory_opt) {
  // SETUP FOR Spyre TENSOR
  at::detail::check_size_nonnegative(size);
  const auto scalar_type = c10::dtype_or_default(dtype_opt);
  caffe2::TypeMeta dtype = c10::scalarTypeToTypeMeta(scalar_type);
  c10::Device device = device_opt.value_or(
      c10::impl::VirtualGuardImpl{c10::DeviceType::PrivateUse1}.getDevice());
  DEBUGINFO("Tensor info on CPU (Size:", size, ", Stride: ", stride,
            ", dtype: ", dtype, ") to be mapped onto device ", device);
  auto device_layout = SpyreTensorLayout(size.vec(), scalar_type);
  size_t size_bytes = get_device_size_in_bytes(device_layout);

  auto spyre_storage_impl = c10::make_intrusive<SpyreStorageImpl>(
      c10::StorageImpl::use_byte_size_t(), size_bytes,
      &SpyreAllocator::instance(),
      /*resizeable=*/true);
  auto spyre_storage = c10::Storage(spyre_storage_impl);

  // Create the Spyre Tensor
  const c10::DeviceGuard device_guard(device);
  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);
  auto tensor = at::detail::make_tensor_base<SpyreTensorImpl>(
      std::move(spyre_storage), pu1_dks, dtype);

  auto tensorImpl = tensor.unsafeGetTensorImpl();
  if (size.size() == 0) {
    std::vector<int64_t> one = {1};
    c10::IntArrayRef tmp_size(one);
    c10::IntArrayRef tmp_stride(one);
    tensorImpl->set_sizes_and_strides(tmp_size, tmp_stride);

  } else {
    tensorImpl->set_sizes_and_strides(size, stride);
  }

  static_cast<SpyreTensorImpl*>(tensorImpl)->spyre_layout = device_layout;
  DEBUGINFO("SpyreTensorLayout: ", device_layout.toString());
  return tensor;
}
at::Tensor spyre_empty_with_layout(c10::IntArrayRef size,
                                   c10::IntArrayRef stride,
                                   c10::ScalarType dtype,
                                   SpyreTensorLayout device_layout) {
  at::detail::check_size_nonnegative(size);
  c10::Device device =
      c10::impl::VirtualGuardImpl{c10::DeviceType::PrivateUse1}.getDevice();
  size_t size_bytes = get_device_size_in_bytes(device_layout);
  auto spyre_storage_impl = c10::make_intrusive<SpyreStorageImpl>(
      c10::StorageImpl::use_byte_size_t(), size_bytes,
      &SpyreAllocator::instance(),
      /*resizeable=*/true);
  auto spyre_storage = c10::Storage(spyre_storage_impl);

  // Create the Spyre Tensor
  const c10::DeviceGuard device_guard(device);
  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);
  auto tensor = at::detail::make_tensor_base<SpyreTensorImpl>(
      std::move(spyre_storage), pu1_dks, c10::scalarTypeToTypeMeta(dtype));

  auto tensorImpl = tensor.unsafeGetTensorImpl();
  tensorImpl->set_sizes_and_strides(size, stride);

  static_cast<SpyreTensorImpl*>(tensorImpl)->spyre_layout = device_layout;
  DEBUGINFO("SpyreTensorLayout: ", device_layout.toString());
  return tensor;
}
at::Tensor spyre_as_strided(const at::Tensor& self, c10::IntArrayRef size,
                            c10::IntArrayRef stride,
                            std::optional<int64_t> storage_offset_) {
  // TODO(aviros): This as is will lead to many errors for views, fail for now
  TORCH_CHECK_NOT_IMPLEMENTED(
      !self.is_privateuseone(),
      "as_strided not implemented for Spyre tensors, implement the caller "
      "using as_strided_with_layout with the proper semantics");
  return at::cpu::as_strided(self, size, stride, storage_offset_);
}

at::Tensor& spyre_set_storage(at::Tensor& result, at::Storage storage,
                              int64_t storage_offset, c10::IntArrayRef size,
                              c10::IntArrayRef stride) {
  DEBUGINFO("set method");
  return at::cpu::set_(result, storage, storage_offset, size, stride);
}

/**
 * This method handles copy between devices. When copying to Spyre, this method
 * marks the tensor to compute on Spyre, but continue to use CPU tensor for now
 * such that when we run an op on the tensor on the Spyre, it will have the
 * proper handle to the Spyre allocation
 */
at::Tensor spyre_copy_from(const at::Tensor& self, const at::Tensor& dst,
                           bool non_blocking) {
  DEBUGINFO("self (", self.scalar_type(), ") is on:", self.device());
  DEBUGINFO("dst (", dst.scalar_type(), ") on:", dst.device());
  at::Storage source_storage;
  at::Storage dest_storage;

  // TODO(tmhoangt): add type conversion node
  TORCH_CHECK(
      self.scalar_type() == dst.scalar_type(),
      "Spyre backend does not support type conversion yet during copy.");
  SpyreStream stream;
  if (dst.is_privateuseone()) {
    stream = getCurrentStream(dst.device());
  } else {
    stream = getCurrentStream(self.device());
  }
  stream.copy_async(self, dst);
  if (!non_blocking) {
    stream.synchronize();
  }
  return dst;
}

at::Tensor to_with_layout(const at::Tensor& self,
                          SpyreTensorLayout device_layout) {
  DEBUGINFO(
      "Tensor info on CPU (Size:", self.sizes(), ", Stride: ", self.strides(),
      ", dtype: ", c10::typeMetaToScalarType(self.dtype()),
      ") and to be mapped onto device ",
      c10::impl::VirtualGuardImpl{c10::DeviceType::PrivateUse1}.getDevice(),
      " with layout ", device_layout.toString());
  auto dst = spyre_empty_with_layout(self.sizes(), self.strides(),
                                     c10::typeMetaToScalarType(self.dtype()),
                                     device_layout);
  return spyre_copy_from(self, dst, false);
}

at::Tensor empty_with_layout(
    c10::IntArrayRef size, SpyreTensorLayout device_layout,
    std::optional<c10::ScalarType> dtype_opt,
    std::optional<c10::Layout> layout_opt,
    std::optional<c10::Device> device_opt, std::optional<bool> pin_memory_opt,
    std::optional<c10::MemoryFormat> memory_format_opt) {
  c10::Device device = device_opt.value_or(
      c10::impl::VirtualGuardImpl{c10::DeviceType::PrivateUse1}.getDevice());
  DEBUGINFO("shape=", size, " on Spyre ", device);
  const auto dtype = c10::dtype_or_default(dtype_opt);
  TORCH_CHECK(device.is_privateuseone());
  TORCH_CHECK(c10::layout_or_default(layout_opt) == c10::Layout::Strided,
              "Non strided layout not supported");
  TORCH_CHECK(!c10::pinned_memory_or_default(pin_memory_opt),
              "Pin memory can only be on CPU");
  const c10::DeviceGuard device_guard(device);

  size_t size_bytes = get_device_size_in_bytes(device_layout);
  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);
  auto tensor = at::detail::make_tensor_base<SpyreTensorImpl>(
      c10::Storage(c10::make_intrusive<SpyreStorageImpl>(
          c10::StorageImpl::use_byte_size_t(), size_bytes,
          &SpyreAllocator::instance(),
          /*resizeable=*/true)),
      pu1_dks, c10::scalarTypeToTypeMeta(dtype));

  tensor.unsafeGetTensorImpl()->set_sizes_contiguous(size);
  static_cast<SpyreTensorImpl*>(tensor.unsafeGetTensorImpl())->spyre_layout =
      device_layout;
  DEBUGINFO("SpyreTensorLayout: ", device_layout.toString());
  return tensor;
}

at::Tensor as_strided_with_layout(const at::Tensor& self, c10::IntArrayRef size,
                                  c10::IntArrayRef stride,
                                  std::optional<int64_t> storage_offset_,
                                  SpyreTensorLayout device_layout) {
  // NOTE: This function does not check whether
  // the as_strided info and stl are compatible.
  // This is for the caller of this function to check
  auto storage_offset = storage_offset_.value_or(self.storage_offset());
  auto result = at::detail::make_tensor<SpyreTensorImpl>(
      c10::TensorImpl::VIEW, c10::Storage(self.storage()), self.key_set(),
      self.dtype());
  at::native::setStrided(result, size, stride, storage_offset);
  static_cast<SpyreTensorImpl*>(result.unsafeGetTensorImpl())->spyre_layout =
      device_layout;
  return result;
}

TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {
  m.impl("empty.memory_format", TORCH_FN(spyre_empty));
  m.impl("empty_strided", TORCH_FN(spyre_empty_strided));
  m.impl("as_strided", TORCH_FN(spyre_as_strided));
  m.impl("set_.source_Storage_storage_offset", TORCH_FN(spyre_set_storage));
  m.impl("_copy_from", TORCH_FN(spyre_copy_from));
}

}  // namespace spyre
