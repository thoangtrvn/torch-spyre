# Copyright 2025 The Torch-Spyre Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from contextlib import contextmanager

import torch

from torch._inductor.ir import Reduction, Pointwise
import torch._inductor.lowering as lowering

from typing import Any, Callable, Union

from .constants import MATMUL_REDUCTION_OP, BATCH_MATMUL_OP
from torch_spyre._C import get_elem_in_stick
from torch_spyre.fallbacks import fallback_ops
from .ir import SpyreReduction
from torch._inductor.virtualized import V

# The specific spyre lowerings will be registered into this dictionary
# and merged with the in-tree lowerings when needed
spyre_lowerings: dict[Union[Callable[..., Any], str], Callable[..., Any]] = {}


def register_spyre_lowering(
    op,
    name=None,
    broadcast=False,
    type_promotion_kind=lowering.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT,
    override_return_dtype=None,
    convert_input_to_bool=False,
    lowering_dict=spyre_lowerings,
):
    name = name or op.__name__

    ensure_default_handler(name)

    lowering.register_op_dtype_propagation_rules(
        name=name,
        type_promotion_kind=type_promotion_kind,
        override_return_dtype=override_return_dtype,
    )

    return lowering.register_lowering(
        op,
        broadcast=broadcast,
        type_promotion_kind=type_promotion_kind,
        convert_input_to_bool=convert_input_to_bool,
        lowering_dict=lowering_dict,
    )


# Implicit fallback to an eager op does not become effective when lowering of
# the op is registered by default. Here, we unregister ops that are falling back
# to eager ops
# Note: If an op has a decomposition defined, a lowering is not registered
def unregister_lowering(op, lowering_dict=lowering.lowerings, allow_missing=False):
    for overload in lowering.get_overloads(op):
        if overload in lowering_dict:
            del lowering_dict[overload]
        elif not allow_missing:
            raise RuntimeError(f"lowering of {overload} is not registered")


for op in fallback_ops:
    unregister_lowering(op, allow_missing=True)


# Context manager that enables spyre specific lowerings in addition to PyTorch in-tree lowerings
@contextmanager
def enable_spyre_lowerings():
    saved_intree_lowerings = {}
    try:
        for spyre_lowering_op, spyre_lowering_impl in spyre_lowerings.items():
            if spyre_lowering_op in lowering.lowerings:
                saved_intree_lowerings[spyre_lowering_op] = lowering.lowerings[
                    spyre_lowering_op
                ]
            lowering.lowerings[spyre_lowering_op] = spyre_lowering_impl
        yield
    except Exception as e:
        # TODO: Better error handling here?
        raise e
    finally:
        # Reset the saved in-tree lowerings if needed
        for spyre_lowering_op, spyre_lowering_impl in spyre_lowerings.items():
            if spyre_lowering_op in saved_intree_lowerings:
                lowering.lowerings[spyre_lowering_op] = saved_intree_lowerings[
                    spyre_lowering_op
                ]
            else:
                lowering.lowerings.pop(spyre_lowering_op, None)


def ensure_default_handler(op_name):
    """
    Install a default handler for a custom operator in DefaultHandler.

    DefaultHandler defines handlers for built‑in operators but does not
    automatically create one for custom ops, which leads to warnings like:

      UserWarning: undefined OpHandler.<op_name>, please add missing op schema

    This helper registers a fallback handler to suppress that warning.

    Ref: https://github.com/pytorch/pytorch/blob/v2.9.1/torch/_inductor/ops_handler.py#L745

    TODO: Remove once the handler registration issue is resolved.
    """

    cls = torch._inductor.ops_handler.DefaultHandler
    if op_name not in cls.__dict__:
        method = cls._call_default(op_name)
        setattr(cls, op_name, method)


# @register_spyre_lowering(torch.ops.aten.mm.default)
# def lower_mm(x, y):
#     def inner_fn(index, reduction_index):
#         i0, i1 = index
#         (r0,) = reduction_index
#         return (x_loader([i0, r0]), y_loader([r0, i1]))

#     x = V.graph.get_buffer(x.realize())
#     y = V.graph.get_buffer(y.realize())
#     x_loader = x.make_loader()
#     y_loader = y.make_loader()

#     result = Reduction.create(
#         reduction_type=MATMUL_REDUCTION_OP,
#         input_node=[x, y],
#         device=x.get_device(),
#         dst_dtype=x.get_dtype(),
#         src_dtype=x.get_dtype(),
#         inner_fn=inner_fn,
#         ranges=[x.get_size()[0], y.get_size()[1]],
#         reduction_ranges=[x.get_size()[1]],
#     )

#     result.realize()

#     return result


@register_spyre_lowering(torch.ops.aten.mm.default)
def lower_mm(x, y):
    x = V.graph.get_buffer(x.realize())
    y = V.graph.get_buffer(y.realize())
    x_loader = x.make_loader()
    y_loader = y.make_loader()

    x_size = x.get_size()
    y_size = y.get_size()
    x_ndim = len(x_size)
    y_ndim = len(y_size)

    # Handle 3D input with 2D weight (batched matmul)
    if x_ndim == 3 and y_ndim == 2:

        def inner_fn(index, reduction_index):
            i0, i1, i2 = index  # batch, row, col
            (r0,) = reduction_index
            return (x_loader([i0, i1, r0]), y_loader([r0, i2]))

        result = Reduction.create(
            reduction_type=BATCH_MATMUL_OP,  # Use BATCH_MATMUL_OP for 3D×2D
            input_node=[x, y],
            device=x.get_device(),
            dst_dtype=x.get_dtype(),
            src_dtype=x.get_dtype(),
            inner_fn=inner_fn,
            ranges=[x_size[0], x_size[1], y_size[1]],  # [B, M, N]
            reduction_ranges=[x_size[2]],  # K
        )
    # Standard 2D × 2D matrix multiplication
    elif x_ndim == 2 and y_ndim == 2:

        def inner_fn(index, reduction_index):
            i0, i1 = index
            (r0,) = reduction_index
            return (x_loader([i0, r0]), y_loader([r0, i1]))

        result = Reduction.create(
            reduction_type=MATMUL_REDUCTION_OP,  # Use MATMUL_REDUCTION_OP for 2D×2D
            input_node=[x, y],
            device=x.get_device(),
            dst_dtype=x.get_dtype(),
            src_dtype=x.get_dtype(),
            inner_fn=inner_fn,
            ranges=[x_size[0], y_size[1]],
            reduction_ranges=[x_size[1]],
        )
    else:
        raise ValueError(
            f"Unsupported tensor dimensions for mm: x.shape={x_size}, y.shape={y_size}. "
            f"Expected (2D, 2D) or (3D, 2D), got ({x_ndim}D, {y_ndim}D)"
        )

    result.realize()
    return result


@register_spyre_lowering(torch.ops.aten.bmm.default)
def lower_bmm(x, y):
    x = V.graph.get_buffer(x.realize())
    y = V.graph.get_buffer(y.realize())
    x_loader = x.make_loader()
    y_loader = y.make_loader()
    d3 = len(x.get_size()) == 3
    if d3:

        def inner_fn(index, reduction_index):
            i0, i1, i2 = index
            (r0,) = reduction_index
            tmp1 = x_loader([i0, i1, r0])
            tmp2 = y_loader([i0, r0, i2])
            return (tmp1, tmp2)

        result = Reduction.create(
            reduction_type=BATCH_MATMUL_OP,
            input_node=[x, y],
            device=x.get_device(),
            dst_dtype=x.get_dtype(),
            src_dtype=x.get_dtype(),
            inner_fn=inner_fn,
            ranges=[x.get_size()[0], x.get_size()[1], y.get_size()[2]],  # B, M, N
            reduction_ranges=[x.get_size()[2]],  # K
        )
    else:  # 4d

        def inner_fn(index, reduction_index):
            i0, i1, i2, i3 = index
            (r0,) = reduction_index
            tmp1 = x_loader([i0, i1, i2, r0])
            tmp2 = y_loader([i0, i1, r0, i3])
            return (tmp1, tmp2)

        result = Reduction.create(
            reduction_type=BATCH_MATMUL_OP,
            input_node=[x, y],
            device=x.get_device(),
            dst_dtype=x.get_dtype(),
            src_dtype=x.get_dtype(),
            inner_fn=inner_fn,
            ranges=[
                x.get_size()[0],
                x.get_size()[1],
                x.get_size()[2],
                y.get_size()[-1],
            ],
            reduction_ranges=[x.get_size()[-1]],
        )

    result.realize()
    return result


@register_spyre_lowering(torch.ops.spyre.swap)
def lower_swap(x):
    fn = lowering.ops_wrapper(torch.ops.spyre.swap.__name__)

    def inner_fn(index):
        return fn(x.make_loader()(index))

    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.spyre.slice)
def lower_slice(x):
    fn = lowering.ops_wrapper(torch.ops.spyre.slice.__name__)

    def inner_fn(index):
        return fn(x.make_loader()(index))

    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.spyre.exx2)
def lower_exx2(x, exx2Scale, useZeroMean):
    kwargs = lowering._make_reduction_inner(
        x, axis=[-1], keepdims=True, dtype=x.dtype, override_return_dtype=None
    )
    op_info = {
        "constants": {
            "exx2scale": exx2Scale,
            "useZeroMean": useZeroMean,
        }
    }
    result = SpyreReduction.create(
        reduction_type="exx2",
        input_node=x,
        device=x.get_device(),
        dst_dtype=x.get_dtype(),
        src_dtype=x.get_dtype(),
        inner_fn=kwargs["inner_fn"],
        ranges=x.get_size()[:-1] + [get_elem_in_stick(x.get_dtype())],
        reduction_ranges=kwargs["reduction_ranges"],
        op_info=op_info,
    )
    result.realize()
    return result


@register_spyre_lowering(torch.ops.spyre.layernormnorm)
def lower_layernormnorm(x, mean, norm_mean, weight, bias):
    fn = lowering.ops_wrapper(torch.ops.spyre.layernormnorm.__name__)

    def inner_fn(index):
        loaded_inputs = [
            x.make_loader()(index),
            mean.make_loader()(index),
            norm_mean.make_loader()(index),
        ]
        if weight is not None:
            loaded_inputs.append(weight.make_loader()(index[-1:]))
        if bias is not None:
            loaded_inputs.append(bias.make_loader()(index[-1:]))
        return fn(*loaded_inputs)

    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.spyre.layernormscale)
def lower_layernormscale(x, eps):
    fn = lowering.ops_wrapper(torch.ops.spyre.layernormscale.__name__)

    def inner_fn(index):
        return fn(x.make_loader()(index), eps)

    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.aten.mean.dim)
def lower_mean(x, axis=None, keepdim=False, *, dtype=None):
    kwargs = lowering._make_reduction_inner(
        x, axis=axis, keepdims=keepdim, dtype=x.dtype, override_return_dtype=None
    )
    size = x.get_size()
    denom = torch._inductor.utils.sympy_product(size[i] for i in axis)
    scaling_factor = 1.0 / denom
    op_info = {"constants": {"scaling_factor": scaling_factor}}
    result = SpyreReduction.create(
        reduction_type="mean", input_node=x, op_info=op_info, **kwargs
    )
    result.realize()
    return result


@register_spyre_lowering(torch.ops.spyre.gelu)
def lower_gelu(x, approximate="none"):
    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=lambda index: lowering.ops_wrapper(torch.ops.spyre.gelu.__name__)(
            x.make_loader()(index)
        ),
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.spyre.softplus)
def lower_softplus(x, beta=1.0, threshold=20.0):
    fn = lowering.ops_wrapper(torch.ops.spyre.softplus.__name__)

    def inner_fn(index):
        return fn(x.make_loader()(index), beta, threshold)

    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=inner_fn,
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw


@register_spyre_lowering(torch.ops.spyre.clamp)
def lower_clamp(x, min=None, max=None):
    if min is None:
        min = torch.finfo(torch.float16).min
    if max is None:
        max = torch.finfo(torch.float16).max
    pw = Pointwise.create(
        device=x.get_device(),
        dtype=x.get_dtype(),
        inner_fn=lambda index: lowering.ops_wrapper(torch.ops.spyre.clamp.__name__)(
            x.make_loader()(index), min, max
        ),
        ranges=x.get_size(),
        origin_node=x.get_origin_node(),
        traceback=x.get_traceback(),
    )
    pw.realize()
    return pw
