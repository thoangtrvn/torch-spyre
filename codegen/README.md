# Codegen Module

This module automatically generates and registers custom function implementations for AIU eager mode support.

`generate_and_register_wrappers()` in `codegen/gen.py` can be executed in `setup.py` before installing torch_spyre.

## How does it work?

Codegen basically parses and maps torch functions to sendnn functions in a mostly-automated manner:

1) **Parsing torch functions:** Codegen relies on `RegistrationDeclarations.h` and `Declarations.yaml` files (inside inputs folder) to parse aten functions (torch's native c++ library). These two files are auto generated by PyTorch during its build and help us to understand the details of each aten operation in its C++ API. `RegistrationDeclarations.h` is only required to see which aten ops are required to be implemented for Spyre (the ones with dispatch=True, default=False). `Declarations.yaml` contains detailed information about schema, arguments/returns properties etc.

2) **Parsing sendnn functions:** Codegen relies on `Metadata.yaml` (inside inputs folder) to infer which functions are supported by sendnn. This file **is manually managed**. Codegen loops over torch functions and for each function, it continues to next steps only if the function is provided in metadata. If the function is not found in metadata, it is skipped. Schemas, arguments and dtypes of sendnn functions are parsed with the utilities provided in `codegen/utils/arg_mapper.py`.

3) **Function templates:** Codegen utilizes the function templates (jinja2 files inside codegen/templates) that are manually implemented for various categories of functions. Currently, most functions are supported with the base template. Codegen relies on `Metadata.yaml` to decide which template to use for each function. For each function that is supported, the relevant template is rendered with template data that is generated with codegen.

4) **Main body:** Codegen assumes that we will have an existing C++ file (`inputs/spyre_torch_ops.cpp`) that is treated as the main body containing include statements and helper functions, as well as some eager implementations that require manual generation, such as the fused matmul add. This file will be filled with the auto-generated code and the output will be written to the file `outputs/spyre_torch_ops.cpp`. This file is included as source in the compilation for torch_spyre to register auto-generated custom implementations.

5) **Skipped/Failed Functions during Codegen:** Here are the possible reasons for a function being skipped in codegen:
- PyTorch does not require registration.
- Sendnn does not provide support (no entry with that function name exists in `Metadata.yaml`)
- Template was not found. (For now I set template_name to "custom" for unsupported ops by codegen even though they are supported by sendnn. Since there is no template with name "custom", those are skipped by codegen. They can be manually implemented of course.)
- Argument mapping has failed.
  - There is an argument with an unsupported dtype (`Dimname` is not supported.)
  - There are extra arguments in sendnn schema that could not be inferred from torch schema, or argument data was not provided in metadata.
  - If there are extra arguments in torch schema, they are ignored or defaulted with the value provided in metadata. A warning is thrown in this case.
- Output shape/stride expression could not be inferred.

![Process Flow for a Function in Codegen](assets/codegen.png "Codegen Flow")

## TODOs (for Fixes/Potential Edge Cases and Improvements):

- Operation overloads including `Dimname` type arguments in torch schemas are skipped.

- Check the correctness of type conversions between torch and sendnn for edge cases:
  - Example: `int` argument in torch may need to be mapped to `int`, `ConstInput`, `TensorShape`
  - Argument and type mapping logic is derived based on sendnn schema parsing and handled in `codegen/utils/arg_mapper.py`

- Codegen requires output shape/stride extraction mode specification in `metadata.yaml` for each operation.
  - Default mode “bypass”, when no mode is specified, assumes that output shape/stride will be derived from first input tensor for that operation.
  - Other modes (“matmul”, “concat”, “transpose”, “reduce”) can be used for relevant operations.
  - If mode is “infer”, symbolic tracing with FakeTensorMode is performed to infer output shape and stride. This mode is limited as it does not work properly for every overload.

- Unsupported ops:
  - First argument not a Tensor: arange, zeros, ones
    - Requires separate templates
  - Others: avg_pool2d, clamp, convolution, index, nll_loss, select, slice, where
    - Op-specific intermediate steps that does not fit in the current codegen logic

- Sub-optimally implemented ops:
  - Operations that are fused or that may be internally decomposed in compiler: addmm, reduce ops
    - A “DummyOp” node related error is thrown in dee_graph_converter.cpp
    - Workaround: Manually implemented in a decomposed way in the frontend, even though codegen supports.
  - Operations that require autograd support: dropout, layer_norm

## Backend Compatibility Issues:

- View operations:
  - View ops (transpose, reshape, slice etc.) results in tensor views with different strides.
    - There is currently no way to provide custom strides to Deeptools side.
- Non-contiguous inputs:
  - Calling operations on non-contiguous input tensors results in unexpected behavior and enforces a copy to a contiguous tensor beforehand.
- Data Stickification:
  - Data conversion is not (and should not be) transparent to frontend but we observe correctness issues in matmul and other operations that operate on input dims/axes such as sum, softmax etc. Even though these are supported with codegen, they are manually implemented to correctly work in more cases for now but still not reliable.
  - For matmul, compiler decides which axis to traverse inputs based on the stick dimension, which changes based on input matrix dimensions. For example, multiplying matrices of sizes 3x5 and 5x7, yields correct results but with 3x7 and 7x5, the second input is traversed in row-major order instead of column-major order.
  - For ops with input dims/axes, the traversal direction again depends on stick dimension. For example, softmax over the dim=0 a 2x3 matrix yields corrects result but not with a 3x2 matrix.
  - Creating a 4-dim tensor where the last dim size is larger than the third dim size may throw deeptools errors. There may be more cases that cause failure in 4-dim tensors.
